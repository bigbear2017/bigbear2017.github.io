<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 机器学习 | To Be Your Salt]]></title>
  <link href="http://bigbear2017.github.io/blog/categories/ji-qi-xue-xi/atom.xml" rel="self"/>
  <link href="http://bigbear2017.github.io/"/>
  <updated>2016-12-22T08:12:04+08:00</updated>
  <id>http://bigbear2017.github.io/</id>
  <author>
    <name><![CDATA[Cao Nannan]]></name>
    <email><![CDATA[sei_michael@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Word2Vec算法介绍一: A Neural Probabilistic Language Model]]></title>
    <link href="http://bigbear2017.github.io/blog/2016/11/22/word2vecsuan-fa-jie-shao/"/>
    <updated>2016-11-22T08:00:10+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2016/11/22/word2vecsuan-fa-jie-shao</id>
    <content type="html"><![CDATA[<h5>版权声明：本文所有内容都是原创，如有转载请注明出处，谢谢。</h5>

<h3>引言</h3>

<p>这篇文章基本上是word2vec的开篇之作，后来的工作也都是在这篇文章的基础上，进行的优化。之所以要做word2vec，目的也是比较明显的。
在没有word2vec之前，如果我们想计算两个句子的相似性，就只能用ngram这种方式来计算相似性。但是这种方式的缺点是，由于词汇量都很大，导致生成的数据向量的维度都很大，并且很稀疏。另外，无法考虑两个词的语义相似性。比如，一只猫正在卧室里走。通过对语料的训练，我们应该可以得到，相似的语句像一只狗正在卧室里走。Word2Vec利用统计语言模型，对上面的问题有了一个初步的解决方法。</p>

<h3>算法结构</h3>

<p>word2vec基本上可以分为这样几步：</p>

<ol>
<li>对于词典中的每一个词，都关联一个特征向量</li>
<li>使用特征向量来表示词组序列的概率函数</li>
<li>利用词组数据来学习 特征向量和概率函数的参数</li>
</ol>


<p>第一步很简单，对于每一个词，我们随机初始化一个特征向量。第二步，这个就是神经网络的设计了。具体的设计，会在下面说明。
第三步，就是通过数据训练神经网络，得到一个合理的特征向量和神经网络的参数。这个就比较简单了。先是用FeedForward计算输出值，再用BackForward 求导计算。</p>

<p>通过上面的方式，我们可以得到两个部分，一个是被压缩的特征向量。另外一个是训练好的神经网络。我们经常使用的是特征向量，因为通过它，我们会发现一些非常有趣的事情。比如，向量(猫) 和 向量(狗)会比较接近。甚至 向量(皇后) + 向量(男人) 和向量(皇帝)很接近。通过这种把词语转换成向量的方式，我们就可以做很多事情。比如聚类，分类，甚至是文档的匹配等等。</p>

<h3>具体的过程</h3>

<p>训练的过程是为了最大化log-likelihood，定义如下：</p>

<p>$$ L = \frac{1}{T} \sum_t \log f(w_t, w_{t-1}, &hellip;, w_{t-n+1}; \theta ) + R(\theta) $$</p>

<p>论文中给出的具体计算过程如下图：</p>

<p><img src="http://www.bigbear2017.com/images/word2vec-network.png" height="500" width="500" alt="word2vec" /></p>

<p>从上面的图我们可以知道：</p>

<p>$$ f(x) = b + Wx + U tanh(d + Hx)$$</p>

<p>where $tanh(x) = \frac{e<sup>x</sup> - e^{-x}}{ e<sup>x</sup> + e^{-x} }$ and $\frac{d} {dx} tanh = 1 - tanh<sup>2</sup> x $
神经网络的输入是$w_t, w<em>{t-1}, &hellip;, w</em>{t-n +1}$，通过mapping matrix C，我们得到每个词相应的vector $C(w_t )$。每一次训练的时候，把输入$w<em>{t-1}, &hellip; , w</em>{t-n+1}$，合并成一个vector，得到我们输入:</p>

<p>$$ x = ( C(w<em>{t-1}), C(w</em>{t-2}), &hellip;, C(w_{t-n+1}))$$</p>

<p>而我们输出层$y_i$是由$w_t$映射到 one-hot encoder得到。但是我们的神经网络的结果可能会大于1。所以作者就在最后一层增加了一个soft max.</p>

<p>$$ P( w_t | w<em>{t-1}, &hellip;, w</em>{t-n +1} )  = \frac{\exp^{y<em>{w_t}}}{\sum</em>{i} \exp^{y_i}}$$</p>

<p>这样，我们就可以训练整个神经网络了。</p>

<h4>计算过程：</h4>

<p>主要的forward过程比较简单，可以很容易完成。首先我们可以把$f(x)$ 分成两步：</p>

<p>$$a = d + Hx;$$</p>

<p>$$f(x) = b + Wx + U tanh(a)$$</p>

<p>back propagation需要计算偏导数会有点麻烦，具体如下：</p>

<p>$$ \frac{\partial f(x) }{\partial b} = 1$$</p>

<p>$$ \frac{\partial f(x)}{\partial W} = x$$</p>

<p>$$ \frac{\partial f(x)}{\partial U} = a$$</p>

<p>$$ \frac{\partial f(x)}{\partial d} =  \frac{\partial f(x)}{\partial a}  \frac{\partial a}{\partial d} = U ( 1 - a^2) * 1$$</p>

<p>$$ \frac{\partial f(x)}{\partial H} =  \frac{\partial f(x)}{\partial a}  \frac{\partial a}{\partial H} = U ( 1 - a^2) * x $$</p>

<h4>一个问题：如何训练C</h4>

<p> 我们知道神经网络可以训练参数，这个很容易理解。但是C作为输入的一部分，如何进行训练呢？其实，应该说word2vec有两层的隐含层。C也是通过训练得到。每一次输入的时候，我们可以把C看成是$C = X$，这样，我们就可以对$C$进行求偏导，具体如下：</p>

<p>$$ \frac{\partial f(x)}{\partial C} =  \frac{\partial f(x)}{\partial a}  \frac{\partial a}{\partial X}  \frac{\partial X}{\partial C} = U ( 1 - a^2) * H + W $$</p>

<h3>Word2Vec的意义</h3>

<p>Word2Vec使我们很轻松的将一个词，转化成一个vector，类似于AutoEncoder，Word2Vec使我们可以很容易提取一些相对准确高维的特征。我个人觉得这也是它不同于其他语言模型的地方。像LDA，也可以提取一些高维的特征，但是效果明显不如Word2Vec。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Facebook CTR Paper]]></title>
    <link href="http://bigbear2017.github.io/blog/2016/11/02/facebook-ctr-paper/"/>
    <updated>2016-11-02T22:17:26+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2016/11/02/facebook-ctr-paper</id>
    <content type="html"><![CDATA[<h5>版权声明：本文所有内容都是原创，如有转载请注明出处，谢谢。</h5>

<p>本文主要是介绍了Facebook点击率团队在做优化的过程中所使用的一些策略。相对于Google的那篇CTR paper，本文也是提出一些新的想法，很值得一读。</p>

<h3>一. Evaluation Metrics</h3>

<p>首先文章讨论了，实验的metrics。主要是以下几个metric：</p>

<ul>
<li><p>Normalized Cross Entropy
$$NE = \frac{-\frac{1}{N} \sum_{i=1}^{N} (\frac{1+ y_i}{2} \log(p_i) + \frac{1 - y_i}{2} \log( 1- p_i) )}{-(p * \log p + (1-p) * \log ( 1-p ))}$$
从公式上来看，NE表示的是entropy over average entropy。论文上说，值越小效果就越好。要明白这个公式，我们可以从简单的开始。比如，只有一个点的时候。首先，我们要明白，分子和分母的负号是可以直接去掉的。另外我们也要明白, 去掉负号以后，分子和分母都始终是一个负值。我们假设background 的ctr是一样的。那么，怎么样的预测才是好的预测呢？当有点击的时候，$p_i$是接近于1的。当没有点击的时候$p_i$是接近于0的。也就是说，分子要接近$\log 1$。由于分母是一个负数，分子中越接近$\log 1$，值就越小。所以，值越小，预测的效果越好。</p></li>
<li><p>Calibration
这也是我们经常使用的目标，copc。如果这个值也接近1，效果越好。</p></li>
<li><p>AUC
在点击率预测里，auc是我们最常用的指标。</p></li>
</ul>


<h3>二. 决策树特征转化</h3>

<p>使用决策树生成新的特征主要有以下几步：</p>

<ol>
<li>利用GBDT训练，生成决策树</li>
<li>利用叶子节点，生成新的特征。假设我们有200颗树，一共有1000个叶子节点。每个叶子节点作为一维特征，如果叶子节点为0，该维度就是0。如果叶子节点为1，该节点就是1。</li>
<li>将生成的新的特征，加入到原来的特征中，训练LR模型。
具体过程如下图：</li>
</ol>


<p><img src="http://www.bigbear2017.pub/images/facebook-gdbt-lr.png" height="500" width="500" alt="gbdt" /></p>

<h3>三. 在线学习</h3>

<p>其实，这个方法Google在他们的paper里面已经很仔细的讲过了，并没有什么新的地方。主要是使用的mchan的那篇文章FPTR</p>

<h3>四. 实时数据连接</h3>

<ol>
<li>window size window size过小，就会丢掉点击。window size过大，内存消耗就会变的很大。</li>
<li>错误处理 如果点击数据流出现问题，那么数据样本的分布就会变了。预估的点击率都会偏低，接近于0。这个时候，我们需要立刻停止线上的模型，并恢复。</li>
</ol>


<h3>五. 参数调整</h3>

<ol>
<li>决策树的数量： 大概在500左右。我想这个也取决于自己的实际数据。我们使用也就是200个树左右。树的数量如果太大，也会导致线上的计算量变大。</li>
<li>Boosting Feature Importance：因为在决策树中，每一个节点形成的时候，就要选择一个最好的特征，去最大地降低error。这样通过把这些降低的值加起来，然后排个序，就能知道哪些特征更重要了。不同树之间，相同的特征会合并在一起。通过这样的方法， 可以看到前10个feature占了50%的importance。后300个feature占了不到1%的importance</li>
<li>历史统计数据。通过上面的boosting feature importance，相对于类别性数据，历史统计数据在模型中的重要性更高。</li>
</ol>


<h3>六. Sampling</h3>

<ol>
<li>Uniform sampling在50%的时候，基本上NE保持不变，到10%的时候，NE有1%的降低。不过这个也取决于我们的数据量。如果数据量比较小的话，就不用了。大的话，50%应该也没什么问题。</li>
<li>Negative Down Sampling，使用了不同的值测了一下。大概在0.025的时候，是最好的。为什么，不知道。</li>
<li>Re-calibration，这个在之前的<a href="http://www.bigbear2017.com/blog/2014/04/19/guan-yu-downsamplingde-jie-shi/">DownSampling</a>文章应该有说了。就不说了。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DecisionTree算法介绍--附代码实现]]></title>
    <link href="http://bigbear2017.github.io/blog/2016/06/27/decisiontreesuan-fa-jie-shao/"/>
    <updated>2016-06-27T23:16:03+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2016/06/27/decisiontreesuan-fa-jie-shao</id>
    <content type="html"><![CDATA[<h2>算法介绍</h2>

<p>决策树作为一种简单的算法，用途还是很广泛的。不仅仅这样，他还能够成为其他算法的基础。比如Random Forest，GBDT, AdaBoosting，等等。在实现上，也是非常的简单，训练的速度也是非常的快。先让我们直观上来感受一下，什么是决策树：</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/c/c6/Manual_decision_tree.jpg" alt="decision tree" /></p>

<h2>模型训练</h2>

<h4>Regression Trees</h4>

<p>从上面，我们知道一棵决策树，是由不同的节点构成的。从上到下，样本被不断地分到不同节点中去，一直到叶子节点。最终叶子上的节点，决定了这个叶子节点的预测值或者预测标签是什么。所以具体的过程如下：</p>

<ol>
<li>假设对于任意的分割变量 $j$ 和分割值$v$， 输入$X$可以被分成：</li>
</ol>


<p>$$ R_1(j,v)  = \{ X | X_j &lt; v \} \text{ and } R_2(j , v) = \{ X | X_j > v \} $$</p>

<ol>
<li>j，v可以利用下面的方式确定：</li>
</ol>


<p>$$\min_{j,v} [ \sum_{x_i \in R_1} ( y_i - c_1)^2 + \sum_{x_i \in R_2} ( y_i - c_2)^2 ] $$</p>

<p>其中: $ c_1 = avg( y_i | x_i \in R_1) $, $c_2 = avg(y_i | x_i \in R2) $</p>

<h4>伪代码</h4>

<p>决策树的想法很简单，就是利用特征构成一颗树，树的左边是子树或者类别，每个节点就是决策点。那问题来了，如何找到每个决策点？稍微想一下的话，可能不难明白：</p>

<pre><code>while height &lt; MaxHeight and currentLevel has Nodes to split
  for each Node in current level : 
    for each feature and possible split in current node:
        find the best split s;
        split the node n according to s;
        if the left node or right node can not be split:
           continue
       split current Node into Left and Right
       put left and right into nextLevelList
  currentLevel = nextLevel
  height = height + 1
</code></pre>

<h4>Classfication Trees</h4>

<p>分类树主要是计算criterion的时候，有些差别。其他的总体上，也没什么变化。</p>

<p>Missclassification Error: $ \sum_{k=1}^K ( 1- p_{mk} ) $</p>

<p>Gini Index: $\sum_{k=1}^K p_{mk}( 1- p_{mk} )$</p>

<p>Cross-entropy: $\sum_{k=1}^K p_{mk} \log p_{mk}$</p>

<h2>算法实现</h2>

<p>这些天没有更新blog，就自己实现了一个简单的decision tree。它真的很简单，什么都没有。没有剪枝，没有优化地选feature。10w的samples，100维的数据20秒左右可以训练好。还有很多可以优化的地方。</p>

<p>code repository: <a href="https://github.com/bigbear2017/skywalker">skywalker</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OWL-QN算法介绍]]></title>
    <link href="http://bigbear2017.github.io/blog/2016/06/07/owl-qnsuan-fa-jie-shao/"/>
    <updated>2016-06-07T00:08:56+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2016/06/07/owl-qnsuan-fa-jie-shao</id>
    <content type="html"><![CDATA[<h5>版权声明：本文所有内容都是原创，如有转载请注明出处，谢谢。</h5>

<h3>算法引言</h3>

<p>我们之前介绍了Logistic Regression。通常情况下，对LR进行正则化，我们都会使用L2 norm，因为求导比较简单。但是L1 norm 相对于L2 norm有2个好处：</p>

<ol>
<li>当很多的特征都不相关，我们仍然可以训练出一个好的模型（Ng 2004）</li>
<li>生成的模型，大多数的参数都是0。后面我们会知道这是为什么。</li>
</ol>


<p>第二个条件对很多线上的任务，是一个非常好的属性。因为很多没必要的参数为0，就可以显著的减少线上的计算量。如果是使用L2 norm，大多数的参数都会是接近于0，比如0.004，虽然很小，但是还是要计算这一位的值。</p>

<p>假设我们的参数为$x \in R^p$, 如果我们了解L1 Norm，我们就知道我们需要minimize 的函数如下:</p>

<p>$$f(x) = \ell(x) + r(x) =  \ell(x) + C \sum_{i=1}^{p} |x_i|$$
其中$\ell(x)$是negative loglikelihood。很明显，这个函数不可导。我们连一阶导数都没办法求出来，更别提二阶导数了。所以，我们是不可能用剃度下降或者牛顿法等等。</p>

<h3>算法思想</h3>

<p>作者是怎么想出来OWL-QN(orthant-wise Limited-memory Quasi-Newton)呢？主要的想法是这样的：</p>

<p><em>如果限制到某一个单一象限，上面的$f(x)$就是可导的。并且呢，他还是与 $r(x)$是线性相关的。因此, $r(x)$的那部分，在二阶导数的时候就为0。这也就是说，f(x)在二阶导数上，只与$\ell(x)$有关，与$r(x)$无关。</em></p>

<p>这样的话，于是我们就有了这样的想法：</p>

<ul>
<li>构造一个接近的二次函数，这个接近的意思是，在某个象限内和原函数是一致的</li>
<li>对构造的二次函数，利用Hessian Matrix，找到下降的梯度方向</li>
<li>根据梯度方向，进行受限制的线性搜索，受限制的线性搜索的意思是与原来的$x$的象限一致</li>
</ul>


<p>首先，我们可以构造下面一个函数:
$$ f(x) = \ell(x) + C\xi ^T x $$
其中$\xi_i$和$x_i$同号，并且$\xi_i \in { 1, 0, -1 }$。这样，这个方程和原来的方程的值，在不变象限的情况下，值是一样的。并且关键的是，现在这个方程，是可导的，可导的。这太美了，可以求极值了。我们就利用这个方程，找到梯度下降的方向。然后进行受限制的线性搜索。</p>

<h4>确定导数</h4>

<p>在确定梯度方向的时候，我们还是要确定函数的一阶导数。对于$x_i \ne 0$，这个很好办，方程是可导，我们可以得到$\partial_i f(x) = \frac{\partial }{\partial x_i}\ell + C\sigma(x_i)$。但是对于$x_i = 0$的情况，就比较麻烦，因为这个时候，方程是不可导的。方程的偏导数定义如下:
$$ \partial_i^{\pm} f(x) = \frac{\partial}{\partial x_i} \ell \pm C $$
根据上面的定义，右导数一定比左导数大。如果左偏导和右偏导符号一样，那么我们就选择那个比较小的。如果符号不同，那么就为0。定义如下：
$$\partial_i f(x)  = \begin{cases}
\partial_i^{-} f(x), if \partial^{-}f(x) \gt 0 \\\
\partial_i^{+} f(x), if \partial^{+}f(x) \lt 0 \\\
0, \text{otherwise}
\end{cases}$$</p>

<h4>确定$\zeta$</h4>

<p>首先，我们知道$\zeta$和$x$是同符号的。所以对于$x_i \ne 0$的时候，我们知道是和$x_i$一样的符号。但是$x_i = 0$的时候，我们怎么办呢？想到line search是加上负梯度方向来的，我们就取负梯度的方向。所以，$\zeta$定义如下：
$$\zeta_i^{k} = \begin{cases}
\sigma(x^k_i), if x_i^k \ne 0 \\\
\sigma(- \partial_i f(x)), if x_i^k = 0
\end{cases} $$</p>

<h4>受限制的线性搜索</h4>

<p>我们需要保证line search 是在同一个象限的。也就是说，如果是在同一象限，我们就保留，如果不在同一象限就取$0$。定义如下:</p>

<p>$$ x^{k+1} = \pi( x^k + \alpha p^k, \zeta^k) $$</p>

<h3>算法合并</h3>

<blockquote><p>Algorithm OWL-QN
chose initial point $x<sup>0</sup>$</p>

<p>S = {}, Y = {}</p>

<p>for k = 0 to maxIters do</p>

<blockquote><p>Compute $v^k = - \partial f(x)$</p>

<p>Compute $d^k = H_k v^k$ using S and Y</p>

<p>$p^k = \pi ( d^k, v^k )$</p>

<p>Find $x^{k+1}$ with constraned line search</p>

<p>If termination satisfied return $x^{k+1}$ end if</p>

<p>update S with $s^k = x^{k+1} - x^{k}$</p>

<p>update Y with $y^k = \partial \ell(x^{k+1}) - \partial \ell(x^k)$</p></blockquote>

<p>end for</p></blockquote>

<p>这里有一步，我们没有进行解释，就是$p^k = \pi ( d^k, v^k )$保留了吧，这个文章里面有注释。</p>

<h3>算法讨论</h3>

<p>我觉得理解这个算法，主要是抓住一个点。就是$|x|$这个函数，在一个象限里面是可导的，连续的。只有在$0$点，那个位置才是不可导的。所以，我们线性搜索的时候，并不是直接根据梯度，而是受限制的梯度下降。如下图，我们不是要跨象限，而是只到$0$。这样，我们大多数的参数也都变成了0。
<img src="http://7xv0xu.com1.z0.glb.clouddn.com/absx.jpg" alt="abs" /></p>

<h3>参考论文:</h3>

<p><a href="http://www.machinelearning.org/proceedings/icml2007/papers/449.pdf">Scalable Training of L1-Regularized Log-Linear Models</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GradientBoosting算法介绍]]></title>
    <link href="http://bigbear2017.github.io/blog/2015/11/24/gradientboostingsuan-fa-jie-shao/"/>
    <updated>2015-11-24T23:45:10+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2015/11/24/gradientboostingsuan-fa-jie-shao</id>
    <content type="html"><![CDATA[<h5>版权声明：本文所有内容都是原创，如有转载请注明出处，谢谢。</h5>

<h3>算法介绍</h3>

<p>由AdaBoosting算法，我们知道了什么是Boosting算法，算法大概的结构如下:</p>

<p>$$ G(x) = \sum_{i = 1}^{M} \alpha_i g_i( x ) $$</p>

<p>其中$g_i(x)$是弱分类器，$\alpha_i$是每个分类器的权重。在每一步中，AdaBoosting利用那些被分错类的点，然后增加权重，重新训练一个新的分类器，并且计算分类器的权重。
那什么是Gradient Boosting呢？类似于AdaBoosting, Gradient Boosting也是一种Boosting的算法，结构和上面一样。只是Gradient Boosting是利用Gradient来重新训练一个新的分类器，然后计算权重。</p>

<h3>算法理解</h3>

<h5>1. 简单的Boosting</h5>

<p>如果我们只有一个简单的分类器$g(x)$，不用AdaBoosting，如何进行Boosting呢？
可能最简单的是用Residual来进行训练，过程如下:</p>

<blockquote><p>for j = 1 to M</p>

<blockquote><p>$ r_{ji} = ( y_i - G_{j-1}(x_i) ) $ //r_j should be a vector, size of N</p>

<p>fit $g_j(x)$ to $r_j$</p>

<p>$\alpha_j = \arg \min_{\alpha} \sum_{i = 1}^n L( y_i, G_{j-1} ( x_i ) + \alpha * g_j(x_i) )  $</p>

<p>$G_j(x) = G_{j-1}(x) + \alpha_j g_j(x) $</p></blockquote></blockquote>

<h5>2. 使用Gradient</h5>

<p>上面的每一步，我们使用了一个Residual $(y_i - G_{j-1}(x_i) )$ 来重新训练模型。这个表面上看是挺好的，每一次我们都在逼近$y$。但实际上，我们真的在减少Loss吗？假设我们使用Squared Error作为Loss. 那么: $L=  \sum_{i=1}^N ( y_i - \sum_{j=1}^M g_j(x_i) )^2$ 。这个是不是让我们想到Least Squared Regression呢？里面的$\beta$是如何更新的呢？因为是minimize loss，所以，我们是每次加上负梯度，更新如下:
$\beta_{j+1} = \beta_{j} - \frac{\partial L}{\partial \beta }$</p>

<p>看到这个，是不是我们好像似乎明白了什么。假如，我们把每一个$g_i(x)$都看成变量，如果我们想$G(x)$更接近$y$，那我们应该如何更新呢？
$G(x) = G (x) - \frac{\partial L}{\partial G(x)}$
从这个角度，我们再来理解$y_i - G_j(x_i)$的话，就可以知道它其实就是$L = (y - G(x))^2$的负梯度($-\frac{\partial L} {\partial G(x)} = - ( y - G(x) ) * -1 = y - G(x) $).</p>

<h4>其他梯度</h4>

<h5>1. Absolute Loss</h5>

<p>Absolute loss的定义如下:</p>

<p>$$ L(y, G) = |y - G|$$</p>

<p>梯度如下:
$$ \frac{\partial L}{\partial G} =
\begin{cases}
y - G(x) &amp; if y \ge G(x) \\\
G(x) - y &amp; otherwise
\end{cases} $$</p>

<h5>2. Huber Loss</h5>

<p>$$ L_\delta(y, f(x)) =
\begin{cases}
 \frac{1}{2}(y - f(x))^2                   &amp; \textrm{for} |y - f(x)| \le \delta, \\\
 \delta\, |y - f(x)| - \frac{1}{2}\delta^2 &amp; \textrm{otherwise.}
\end{cases} $$</p>

<p>所以
$$
\frac{\partial L_\delta(y, G(x))} {\partial G} = \begin{cases}
y - G(x)                 &amp; \textrm{for} |y - G(x)| \le \delta, \\\
 \delta sign( y - f(x) )  &amp; \textrm{otherwise.}
\end{cases} $$</p>

<h4>算法过程</h4>

<blockquote><p>Input: training set ${(x_i, y_i)_{i=1}^n}$</p>

<p>初始化learner function $G_0$为常数</p>

<p>for j = 1 to M</p>

<blockquote><p>$ r_{ji} = -\frac{\partial L} {\partial G_{j-1}(x_i)} $ //$r_j$ should be a vector, size of N</p>

<p>fit $g_j(x)$ to $r_j$</p>

<p>$\alpha_j = \arg \min_{\alpha} \sum_{i = 1}^n L( y_i, G_{j-1} ( x_i ) + \alpha * g_j(x_i) )  $</p>

<p>$G_j(x) = G_{j-1}(x) + \alpha_j g_j(x) $</p></blockquote></blockquote>

<p>初始化learner function $F_0$为常数
计算negative gradient
以negative gradient为目标进行训练得到一个简单的learner
找到最优的常数得到新的更强的learner</p>
]]></content>
  </entry>
  
</feed>
