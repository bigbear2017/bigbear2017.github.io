
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>GradientBoosting算法介绍 | To Be Your Salt</title>

	<meta name="author" content="Cao Nannan"> 
	
	<meta name="description" content="我是一个基督徒，喜欢机器学习，Linux, 编程方面的知识。目前从事点击率预估， 文字链匹配广告，以及想关的工作。 生活中，我喜欢上帝，喜欢耶稣基督，也喜欢读圣经。"> <meta name="keywords" content="">

	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="To Be Your Salt" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="/stylesheets/font-awesome.min.css" rel="stylesheet" type="text/css">
	
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	
</head>



<body>
	<header id="header"><div class="inner">
  <ul class="navigation">
    <li><a href="/"><h1>To Be Your Salt</h1></a></li>
    <li><a href="/" class="is-selected">Blog</a></li>
    <li><a href="http://github.com/bigbear2017" target="blank">Github</a></li>
  </ul>
</div>
</header>

	<div id="content" class="inner"><article class="post">
	<h2 class="title">GradientBoosting算法介绍</h2>
	<div class="meta">
		<div class="date">








  



<time datetime="2015-11-24T23:45:10+08:00" pubdate data-updated="true">Nov 24th, 2015</time></div>
	</div>
	<div class="entry-content"><h5>版权声明：本文所有内容都是原创，如有转载请注明出处，谢谢。</h5>

<h3>算法介绍</h3>

<p>由AdaBoosting算法，我们知道了什么是Boosting算法，算法大概的结构如下:</p>

<p>$$ G(x) = \sum_{i = 1}^{M} \alpha_i g_i( x ) $$</p>

<p>其中$g_i(x)$是弱分类器，$\alpha_i$是每个分类器的权重。在每一步中，AdaBoosting利用那些被分错类的点，然后增加权重，重新训练一个新的分类器，并且计算分类器的权重。
那什么是Gradient Boosting呢？类似于AdaBoosting, Gradient Boosting也是一种Boosting的算法，结构和上面一样。只是Gradient Boosting是利用Gradient来重新训练一个新的分类器，然后计算权重。</p>

<h3>算法理解</h3>

<h5>1. 简单的Boosting</h5>

<p>如果我们只有一个简单的分类器$g(x)$，不用AdaBoosting，如何进行Boosting呢？
可能最简单的是用Residual来进行训练，过程如下:</p>

<blockquote><p>for j = 1 to M</p>

<ul>
<li><p>$ r_{ji} = ( y_i - G_{j-1}(x_i) ) $ //r_j should be a vector, size of N</p></li>
<li><p>fit $g_j(x)$ to $r_j$</p></li>
<li><p>$\alpha_j = \arg \min_{\alpha} \sum_{i = 1}^n L( y_i, G_{j-1} ( x_i ) + \alpha * g_j(x_i) )  $</p></li>
<li><p>$G_j(x) = G_{j-1}(x) + \alpha_j g_j(x) $</p></li>
</ul>
</blockquote>

<h5>2. 使用Gradient</h5>

<p>上面的每一步，我们使用了一个Residual $(y_i - G_{j-1}(x_i) )$ 来重新训练模型。这个表面上看是挺好的，每一次我们都在逼近$y$。但实际上，我们真的在减少Loss吗？假设我们使用Squared Error作为Loss. 那么: $L=  \sum_{i=1}^N ( y_i - \sum_{j=1}^M g_j(x_i) )^2$ 。这个是不是让我们想到Least Squared Regression呢？里面的$\beta$是如何更新的呢？因为是minimize loss，所以，我们是每次加上负梯度，更新如下:
$\beta_{j+1} = \beta_{j} - \frac{\partial L}{\partial \beta }$</p>

<p>看到这个，是不是我们好像似乎明白了什么。假如，我们把每一个$g_i(x)$都看成变量，如果我们想$G(x)$更接近$y$，那我们应该如何更新呢？
$G(x) = G (x) - \frac{\partial L}{\partial G(x)}$
从这个角度，我们再来理解$y_i - G_j(x_i)$的话，就可以知道它其实就是$L = (y - G(x))^2$的负梯度($-\frac{\partial L} {\partial G(x)} = - ( y - G(x) ) * -1 = y - G(x) $).</p>

<h4>其他梯度</h4>

<h5>1. Absolute Loss</h5>

<p>Absolute loss的定义如下:</p>

<p>$$ L(y, G) = |y - G|$$</p>

<p>梯度如下:
$$ \frac{\partial L}{\partial G} =
\begin{cases}
y - G(x) &amp; if y \ge G(x) \\\
G(x) - y &amp; otherwise
\end{cases} $$</p>

<h5>2. Huber Loss</h5>

<p>$$ L_\delta(y, f(x)) =
\begin{cases}
 \frac{1}{2}(y - f(x))^2                   &amp; \textrm{for} |y - f(x)| \le \delta, \\\
 \delta\, |y - f(x)| - \frac{1}{2}\delta^2 &amp; \textrm{otherwise.}
\end{cases} $$</p>

<p>所以
$$
\frac{\partial L_\delta(y, G(x))} {\partial G} = \begin{cases}
y - G(x)                 &amp; \textrm{for} |y - G(x)| \le \delta, \\\
 \delta sign( y - f(x) )  &amp; \textrm{otherwise.}
\end{cases} $$</p>

<h4>算法过程</h4>

<blockquote><p>Input: training set ${(x_i, y_i)_{i=1}^n}$</p>

<p>初始化learner function $G_0$为常数</p>

<p>for j = 1 to M</p>

<p> &lsquo;  &rsquo;  $ r_{ji} = -\frac{\partial L} {\partial G_{j-1}(x_i)} $ //$r_j$ should be a vector, size of N</p>

<p> &lsquo;  &rsquo;  fit $g_j(x)$ to $r_j$</p>

<p> &lsquo;  &rsquo;  $\alpha_j = \arg \min_{\alpha} \sum_{i = 1}^n L( y_i, G_{j-1} ( x_i ) + \alpha * g_j(x_i) )  $</p>

<p> &lsquo;  &rsquo;  $G_j(x) = G_{j-1}(x) + \alpha_j g_j(x) $</p></blockquote>

<p>初始化learner function $F_0$为常数
计算negative gradient
以negative gradient为目标进行训练得到一个简单的learner
找到最优的常数得到新的更强的learner</p>
</div>


<div class="sharing">
  
  
  
</div>
</article>
</div>
	<footer id="footer" class="inner"><script type="text/x-mathjax-config"> 
MathJax.Hub.Config({
	tex2jax: {
		inlineMath: [ ['$','$'] ],
		displayMath: [ ['$$','$$'] ],
	},
	processEscapes: true
});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
<div class="aboutme">
<hr>
  <div class="profile">
    <a href="http://twitter.com/"><img class="icon" src=""></a>

    <h3>Cao Nannan</h3>
    <p><a href="http://twitter.com/">@</a></p>

    <div class="codes">
      <h4>Codes</h4>
      <div class="github"><a href="https://github.com/bigbear2017">github.com/bigbear2017</a></div>
    </div>
  </div>

  <div class="float_clear"></div>

</div>
Copyright &copy; 2016
 Cao Nannan 
<br>
Powered by Octopress.



</footer>
	

</body>
</html>
