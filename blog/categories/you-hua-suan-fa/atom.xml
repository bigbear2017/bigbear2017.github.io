<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 优化算法 | To Be Your Salt]]></title>
  <link href="http://bigbear2017.github.io/blog/categories/you-hua-suan-fa/atom.xml" rel="self"/>
  <link href="http://bigbear2017.github.io/"/>
  <updated>2016-11-02T22:27:48+08:00</updated>
  <id>http://bigbear2017.github.io/</id>
  <author>
    <name><![CDATA[Cao Nannan]]></name>
    <email><![CDATA[sei_michael@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OWL-QN算法介绍]]></title>
    <link href="http://bigbear2017.github.io/blog/2016/06/07/owl-qnsuan-fa-jie-shao/"/>
    <updated>2016-06-07T00:08:56+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2016/06/07/owl-qnsuan-fa-jie-shao</id>
    <content type="html"><![CDATA[<h5>版权声明：本文所有内容都是原创，如有转载请注明出处，谢谢。</h5>

<h3>算法引言</h3>

<p>我们之前介绍了Logistic Regression。通常情况下，对LR进行正则化，我们都会使用L2 norm，因为求导比较简单。但是L1 norm 相对于L2 norm有2个好处：</p>

<ol>
<li>当很多的特征都不相关，我们仍然可以训练出一个好的模型（Ng 2004）</li>
<li>生成的模型，大多数的参数都是0。后面我们会知道这是为什么。</li>
</ol>


<p>第二个条件对很多线上的任务，是一个非常好的属性。因为很多没必要的参数为0，就可以显著的减少线上的计算量。如果是使用L2 norm，大多数的参数都会是接近于0，比如0.004，虽然很小，但是还是要计算这一位的值。</p>

<p>假设我们的参数为$x \in R^p$, 如果我们了解L1 Norm，我们就知道我们需要minimize 的函数如下:</p>

<p>$$f(x) = \ell(x) + r(x) =  \ell(x) + C \sum_{i=1}^{p} |x_i|$$
其中$\ell(x)$是negative loglikelihood。很明显，这个函数不可导。我们连一阶导数都没办法求出来，更别提二阶导数了。所以，我们是不可能用剃度下降或者牛顿法等等。</p>

<h3>算法思想</h3>

<p>作者是怎么想出来OWL-QN(orthant-wise Limited-memory Quasi-Newton)呢？主要的想法是这样的：</p>

<p><em>如果限制到某一个单一象限，上面的$f(x)$就是可导的。并且呢，他还是与 $r(x)$是线性相关的。因此, $r(x)$的那部分，在二阶导数的时候就为0。这也就是说，f(x)在二阶导数上，只与$\ell(x)$有关，与$r(x)$无关。</em></p>

<p>这样的话，于是我们就有了这样的想法：</p>

<ul>
<li>构造一个接近的二次函数，这个接近的意思是，在某个象限内和原函数是一致的</li>
<li>对构造的二次函数，利用Hessian Matrix，找到下降的梯度方向</li>
<li>根据梯度方向，进行受限制的线性搜索，受限制的线性搜索的意思是与原来的$x$的象限一致</li>
</ul>


<p>首先，我们可以构造下面一个函数:
$$ f(x) = \ell(x) + C\xi ^T x $$
其中$\xi_i$和$x_i$同号，并且$\xi_i \in { 1, 0, -1 }$。这样，这个方程和原来的方程的值，在不变象限的情况下，值是一样的。并且关键的是，现在这个方程，是可导的，可导的。这太美了，可以求极值了。我们就利用这个方程，找到梯度下降的方向。然后进行受限制的线性搜索。</p>

<h4>确定导数</h4>

<p>在确定梯度方向的时候，我们还是要确定函数的一阶导数。对于$x_i \ne 0$，这个很好办，方程是可导，我们可以得到$\partial_i f(x) = \frac{\partial }{\partial x_i}\ell + C\sigma(x_i)$。但是对于$x_i = 0$的情况，就比较麻烦，因为这个时候，方程是不可导的。方程的偏导数定义如下:
$$ \partial_i^{\pm} f(x) = \frac{\partial}{\partial x_i} \ell \pm C $$
根据上面的定义，右导数一定比左导数大。如果左偏导和右偏导符号一样，那么我们就选择那个比较小的。如果符号不同，那么就为0。定义如下：
$$\partial_i f(x)  = \begin{cases}
\partial_i^{-} f(x), if \partial^{-}f(x) \gt 0 \\\
\partial_i^{+} f(x), if \partial^{+}f(x) \lt 0 \\\
0, \text{otherwise}
\end{cases}$$</p>

<h4>确定$\zeta$</h4>

<p>首先，我们知道$\zeta$和$x$是同符号的。所以对于$x_i \ne 0$的时候，我们知道是和$x_i$一样的符号。但是$x_i = 0$的时候，我们怎么办呢？想到line search是加上负梯度方向来的，我们就取负梯度的方向。所以，$\zeta$定义如下：
$$\zeta_i^{k} = \begin{cases}
\sigma(x^k_i), if x_i^k \ne 0 \\\
\sigma(- \partial_i f(x)), if x_i^k = 0
\end{cases} $$</p>

<h4>受限制的线性搜索</h4>

<p>我们需要保证line search 是在同一个象限的。也就是说，如果是在同一象限，我们就保留，如果不在同一象限就取$0$。定义如下:</p>

<p>$$ x^{k+1} = \pi( x^k + \alpha p^k, \zeta^k) $$</p>

<h3>算法合并</h3>

<blockquote><p>Algorithm OWL-QN
chose initial point $x<sup>0</sup>$</p>

<p>S = {}, Y = {}</p>

<p>for k = 0 to maxIters do</p>

<blockquote><p>Compute $v^k = - \partial f(x)$</p>

<p>Compute $d^k = H_k v^k$ using S and Y</p>

<p>$p^k = \pi ( d^k, v^k )$</p>

<p>Find $x^{k+1}$ with constraned line search</p>

<p>If termination satisfied return $x^{k+1}$ end if</p>

<p>update S with $s^k = x^{k+1} - x^{k}$</p>

<p>update Y with $y^k = \partial \ell(x^{k+1}) - \partial \ell(x^k)$</p></blockquote>

<p>end for</p></blockquote>

<p>这里有一步，我们没有进行解释，就是$p^k = \pi ( d^k, v^k )$保留了吧，这个文章里面有注释。</p>

<h3>算法讨论</h3>

<p>我觉得理解这个算法，主要是抓住一个点。就是$|x|$这个函数，在一个象限里面是可导的，连续的。只有在$0$点，那个位置才是不可导的。所以，我们线性搜索的时候，并不是直接根据梯度，而是受限制的梯度下降。如下图，我们不是要跨象限，而是只到$0$。这样，我们大多数的参数也都变成了0。
<img src="http://7xv0xu.com1.z0.glb.clouddn.com/absx.jpg" alt="abs" /></p>

<h3>参考论文:</h3>

<p><a href="http://www.machinelearning.org/proceedings/icml2007/papers/449.pdf">Scalable Training of L1-Regularized Log-Linear Models</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Newton Method, BFGS和LBFGS算法介绍]]></title>
    <link href="http://bigbear2017.github.io/blog/2014/09/12/newton-method/"/>
    <updated>2014-09-12T21:49:55+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2014/09/12/newton-method</id>
    <content type="html"><![CDATA[<h5>版权声明：本文所有内容都是原创，如有转载请注明出处，谢谢。</h5>

<p>本文主要介绍Newton Method，BFGS, LBFGS 算法，最重要的是算法推导的过程，以及这三个算法的比较。</p>

<h2>Newton Method</h2>

<p>假设$y = f(x)$，为了求方程$f(x) = 0$的解，我们可以用下面迭代的方式：</p>

<p>$$ x_{n+1} = x_n - \frac{ f(x_n) } { f^{\prime} (x_n) } $$</p>

<p>比如$ f(x) = x^{2} - 5 $，我们可以这么来求：</p>

<p>$$ x_1 = 5, f(5) = 20, f^{\prime}(5) = 10, x_2 = 5 - 20 / 10 = 3 $$
$$x_2 = 3, f(3) = 4, f^{\prime}(3) = 6, x_3 = 3 - 4/6 = 7/3 $$
$$x_3 = 7/3, f(7/3) = 4/9, f^{\prime}(7/3) = 14/3, x_4 = 7/3 - 2/21 = 47/21 $$</p>

<p>这样$ 47/21 $ 已经非常接近5的平方根了。为什么可以这么计算呢？证明如下：
根据泰勒展开式，$ y = f(x) $ 在 $x_n$处的展开为：</p>

<p>$$ y = f^{\prime}(x_n) ( x_{n+1} - x_n ) + f(x_n) $$</p>

<p>如果$ y = 0 $, 那么可得如下结果：</p>

<p>$$ x_{n+1} = x_n - f(x_n) / f^{\prime}(x_n) $$</p>

<p>在许多机器学习的算法中，我们经常可能碰到的是另外一种形式，比如我们想最大化log likelihood函数。
这个时候，我们会对函数先求一个导，导数$f^{\prime}(x) = 0$的地方一般就是极值点。因此这个时候，我们想求的是
$ f^{\prime}(x) = 0 $，而不是$ f(x) = 0 $。如果求$ f^{\prime}(x) = 0 $，则方程的解如下：</p>

<p>$$ x_{n+1} = x_n - \frac{f^{\prime}(x)}{f^{\prime \prime}(x)} $$</p>

<p>但是因为这个方法要求函数的二阶导数，就像在logistic regression中说的一样。求出来的Hessian Matrix实在太大，
求逆也非常的麻烦。因此就有人想到了更为简单的算法，LBFGS就是其中一个比较好的方法，我们先从BFGS算法说起。</p>

<h2>BFGS</h2>

<p>BFGS算法是由Broyden, Fletcher, Goldfarb 和 Shanno四个人共同提出的。这里我们把他们的名字列出来，纪念一下。:)
由最简单的Line Search的方法，我们知道对于求方程$ f(x) = 0 $的解，我们可以进行如下迭代：</p>

<p>$$ x_{k+1} = x_k + \alpha_k \rho_k $$</p>

<p>其中 $\alpha_k$ 为步长，$\rho_k$为迭代方向。根据泰勒展开，我们可以把$f(x)$写成下面格式：</p>

<p>$$ f(x_{k+1}) = f(x_k) + f^{\prime}(x_k)(x_{k+1} - x_k) + \frac{1}{2} (x_{k+1} - x_k)^{T} f^{\prime \prime}(x_k) ( x_{k+1} - x_k ) $$</p>

<p>上面的函数，也可以写成$ \rho $ 的函数，如下：</p>

<p>$$ m_k(\rho) = f_k + f^{\prime T}_k \rho + \frac{1}{2} \rho^{T} B_k \rho $$</p>

<p>其中$f_k$为$f(x_k)$, $f^{\prime}_k$为$f(x)$在$x_k$处的导数。根据上面的式子，当$\rho = 0$的时候，</p>

<p>$$ m_k(0) =  f_k, m^{\prime}_k (0) = f^{\prime T}_k $$</p>

<p>因此在任何一点$x_k$，$f(x_k)$都可以写成$m_k$的形式，方程的曲线也应该是相同的。根据上面的定义，我们可以得到方程在
$x_{k+1}$处，$m_{k+1}$的表达式为:</p>

<p>$$ m_{k+1}(\rho) = f_{k+1} + f^{\prime T}_{k+1} \rho + \frac{1}{2} \rho^{T} B_{k+1} \rho $$</p>

<p>根据这个表达式，$m_{k+1}$ 这个表达式在$x_k$处的导数为:</p>

<p>$$ f^{\prime}(x_k) = m^{\prime}_{k+1}(-\alpha_k \rho_k) = f^{\prime T}_{k+1} - \alpha_k B_{k+1} \rho_k $$</p>

<p>所以得到：</p>

<p>$$ B_{k+1} \alpha_k \rho_k = f^{\prime}_{k+1} - f^{\prime}_k $$</p>

<p>为了简化，定义如下:</p>

<p>$$ s_k = x_{k+1} - x_k = \alpha_k \rho_k, y_{k} = f^{\prime}_{k+1} - f^{\prime}_k $$</p>

<p>所以上面的结果可以写成:</p>

<p>$$ B_{k+1} s_k = y_k $$</p>

<p>假设$ H<em>{k+1} $为$B</em>{k+1}$的逆矩阵，那么我们可以得到:</p>

<p>$$ H_{k+1} y_k = s_k $$</p>

<p>利用$H_k$，$H_{k+1}$可以通过下面的方式获得:</p>

<p>$$ \min_H || H - H_{k+1} || $$</p>

<p>subject to $ H = H^{T}, Hy_k = s_k $.这样我们可以得到$ H_{k+1} $的解为：</p>

<p>$$ H_{k+1} = (I - p_k s_k y^{T}_k)H_k(I-p_k y_k s^{T}_k) + p_k s_k s^{T}_k$$</p>

<p>其中$p_k = \frac{1}{y^{T}_k s_k}$.到这里BFGS的解法就算明白了。相对于Newton Method，BFGS方法不用
计算Hessian Matrix的逆矩阵。但是它仍然要计算一个matrix，所以内存的占用仍然是一样的。具体算法如下：</p>

<blockquote><p>while $|| f^{\prime}_k||$ > $\epsilon$</p>

<blockquote><p>compute search direction</p>

<p>$ p_k = -H_k f^{\prime}_k $</p>

<p>$ x_{k+1} = x_k + \alpha_k p_k $ where $\alpha_k$ is computed from a line search</p>

<p>procedure to satisfy the Wolfe conditions.</p>

<p>Define $ s_k = x_{k+1} - x_k $ and $ y_k = f^{\prime}_{k+1} - f^{\prime}_k $</p>

<p>Compute $H_{k+1}$ with above formula.</p></blockquote>

<p>$k = k + 1$</p>

<p>end( while )</p></blockquote>

<h3>LBFGS</h3>

<p>根据上面的推导，BFGS算法可以写成下面的格式:</p>

<p>$$ x_{k+1} = x_{k} - \alpha_k H_k f^{\prime}_k(x) $$</p>

<p>其中$\alpha_k$为步长，$H_k$可以写成下面的格式:</p>

<p>$$ H_{k+1} = V^{T}_k H_{k} V_k + p_k s_k s^{T}_k $$</p>

<p>其中$p_k = \frac{1}{y^{T}_k s_k}$, $V_k = I - p_k y_k s^{T}_k $, and $s_k = x_{k+1} - x_k$,$y_k = f^{\prime}_{k+1} - f^{\prime}_{k}$.</p>

<p>由于计算$H_{k+1}$相当的麻烦，我们可以存储$m$个最近的$y_i, s_i$，然后来近似当前的$H_{k+1}$. 根据$H_{k+1}$的定义，我们不难发现这是一个递归的定义。在所有计算的过程中，其实就只有$H_0$这么一个矩阵，其他的全部是向量。如果我们使用一个对角矩阵来近似$H_0$，那么所有的计算都是可以用向量来表示。这个也是为什么它叫limited memory BFGS。</p>

<blockquote><p>calculate the new direction $\rho_k$</p>

<p>$q = f^{\prime}(x_k)$</p>

<p>for $ i = k-1 &hellip; k-m $</p>

<blockquote><p>$\alpha_i = p_i s^T_i q $</p>

<p>$ q = q - \alpha_i y_i $</p></blockquote>

<p>end for</p>

<p>$ r = H_0 q $</p>

<p>for $ i = k-m &hellip; k-1 $</p>

<blockquote><p> $\beta = p_i y^T_i r $</p>

<p> $ r = r + s_i(\alpha_i - \beta) $</p></blockquote>

<p>end for</p>

<p>return r which is $ H_{k}*f^{\prime}_k $</p></blockquote>

<h2>L-BFGS algorithm</h2>

<blockquote><p>choose starting point $x_0$, integer $m > 0$;</p>

<p>k = 0;</p>

<p>repeat</p>

<blockquote><p>Choose $H_0$</p>

<p>Compute $\rho_k = - H_{k}*f^{\prime}_k$</p>

<p>Compute $ x_{k+1} = x_k + \alpha_k \rho_k$, where $\alpha_k$ is the step length which statisfy the Wolfe conditions.</p>

<p>if $k > m$, Discard the vector pair ${ s_{k-m}, y_{k-m} }$ from storage.</p>

<p>Compute and save $s_k = x_{k+1} - x_{k}, y = f^{\prime}_{k+1} - f^{\prime}_k$</p>

<p>k = k+1</p></blockquote></blockquote>

<h3>参考资料</h3>

<p><a href="http://en.wikipedia.org/wiki/Newton%27s_method">http://en.wikipedia.org/wiki/Newton%27s_method</a></p>

<p>Numerical Optimization, Chapter 6, 7.</p>
]]></content>
  </entry>
  
</feed>
