
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>SVM算法介绍 | To Be Your Salt</title>

	<meta name="author" content="Cao Nannan"> 
	
	<meta name="description" content="我是一个基督徒，喜欢机器学习，Linux, 编程方面的知识。目前从事点击率预估， 文字链匹配广告，以及想关的工作。 生活中，我喜欢上帝，喜欢耶稣基督，也喜欢读圣经。"> <meta name="keywords" content="">

	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="To Be Your Salt" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="/stylesheets/font-awesome.min.css" rel="stylesheet" type="text/css">
	
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	
</head>



<body>
	<header id="header"><div class="inner">
  <ul class="navigation">
    <li><a href="/"><h1>To Be Your Salt</h1></a></li>
    <li><a href="/" target="blank">Blog</a></li>
    <li><a href="http://github.com/bigbear2017" target="blank">Github</a></li>
    <li><a href="/blog/2014/03/26/zi-wo-jie-shao/" target="blank"> About ME</a></li>
  </ul>
</div>
</header>

	<div id="content" class="inner"><article class="post">
	<h2 class="title">SVM算法介绍</h2>
	<div class="meta">
		<div class="date">








  



<time datetime="2015-05-22T19:35:43+08:00" pubdate data-updated="true">May 22nd, 2015</time></div>
	</div>
	<div class="entry-content"><h5>版权声明：本文所有内容都是原创，如有转载请注明出处，谢谢。</h5>

<h3>引言</h3>

<p>在分类和回归的算法中，svm可以说是一个非常经典的算法。尤其是在文本分类，脸部识别以及图片聚类问题上，都有很广泛的应用。随着深度学习的兴起，很多的应用上，SVM渐渐地没落了。不过它在数学上，还是相当的优美，并且也求解过程也很经典，非常值得认真地学习。</p>

<h3>Perceptron</h3>

<ul>
<li><p>算法介绍
说到svm，我们可以先从最简单的perceptron说起。Percetron是最简单的一种分类器。给n个点(X, Y), 假设$X \in R^p, Y \in (0, 1)$，如何得到一个分类器$f(x) = \beta * x + \beta_0$，使得所有被错误分类的点，到分类线的距离最小。对于正确分类的点，我们不关心，以为已经分的对了。而对于错误分类的点，总体来说误差越小越好。这样的话，我们就可以得到下面的式子:
$$ D( \beta, \beta_0 ) = - \sum_{i} y_i( \beta x_i + \beta_0 )  $$</p>

<p>根据gradient decent，随机初始化一个$\beta$，然后每次更新$\beta$如下：</p>

<p>$$(\beta, \beta_0 )_{n+1}= ( \beta, \beta_0 )_n + \rho * (y_i x_i, y_i )$$</p>

<p>在扫描几遍数据之后，我们得到一个稳定的$\beta, \beta_0$就可以了。</p></li>
<li><p>算法问题</p>

<ol>
<li>算法有无数多个解。每次得到的解都是不一样的。随着初始化，数据的输入，都会发生变化。</li>
<li>求解的步数可能很长，gap越小越难收敛</li>
<li>如果数据是不可分的，那么算法是不会收敛的。</li>
</ol>
</li>
</ul>


<h3>Best Seperating Hyperplane</h3>

<p>在上面求解的过程中，那怎么才是最好的seperating hyperplane呢？SVM提出了一个观点，任何一个类中的点到分类线的距离都是最大的，这个就是最好的。
比如下面这图:</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Svm_separating_hyperplanes_%28SVG%29.svg/512px-Svm_separating_hyperplanes_%28SVG%29.svg.png" alt="svm1{300x300}" /></p>

<p>所以从上面，我们可以得到:</p>

<p>$$
\begin{matrix}<br/>
\max_{\beta  \ \ \beta_0 } M  \\
subject \ \ to \ \ \frac{y_i ( \beta x_i + \beta_0  )} {||\beta||} \ge M
\end{matrix}<br/>
$$</p>

<p>从上面的式子，我们知道，对于 $ || \beta || $，我们取任何值，它都不会影响点到线的距离，因为我们已经做了归一化了.那么我们就取$ || \beta || = \frac{1}{M} $ 好了, 这样我们可以得到:</p>

<p>$$
\begin{matrix}<br/>
\max_{\beta  \ \ \beta_0 }  \frac{1}{ ||\beta || } \\
subject \ \ to \ \ y_i ( \beta x_i + \beta_0  ) \ge 1
\end{matrix}<br/>
$$</p>

<p>稍微转化一下，我们可以得到下面的式子:</p>

<p>$$
\begin{matrix}<br/>
\min_{\beta  \ \ \beta_0 }  \frac{1}{2}{ ||\beta ||^2 } \\
subject \ \ to \ \ y_i ( \beta x_i + \beta_0  ) \ge 1
\end{matrix}<br/>
$$</p>

<p>这样的话，我们就可以对上面的式子进行求解了，根据拉格朗日算法，我们可以得到:</p>

<p>$$
L_P = \frac{1}{2}{ ||\beta ||^2 } - \sum_{ i = i}^{n} \alpha_i [ y_i ( \beta x_i + \beta_0  ) - 1 ]
$$</p>

<p>对$\beta, \beta_0$，分别求偏导，我们可以得到:</p>

<p>$$
\begin{matrix}<br/>
\frac{\partial L_P} {\partial \beta } = \beta - \sum_{i = 1}^{n} \alpha_i y_i x_i = 0 \\
\frac{\partial L_P} {\partial \beta_0 } = \sum_{i=1}^{n} \alpha_i y_i = 0
\end{matrix}<br/>
$$</p>

<p>代入原来的式子，我们可以得到一个dual的形式:</p>

<p>$$\begin{matrix}<br/>
L_D =  \sum_{i = 1}^{n} \alpha_i - \frac{1}{2} \sum_{i= 1}^{n} \sum_{j =1}^{n} \alpha_i \alpha_j y_i y_j x_i^T x_j  \\
subject \ \ to \ \ \alpha_i \ge 0  \ and \ \sum_{i=1}^{n} \alpha_i y_i  = 0
\end{matrix}$$</p>

<p>从上面的式子，我们可以看到：
1. 如果$\alpha_i >0 $，那么$y_i (\beta x_i + \beta_0) = 1$，那么点就在边界上面
2. 如果$\alpha_i = 0 $，那么点不在边界上面</p>

<h3>Non-Seperable Case</h3>

<p>上面已经讨论了可分的情况，但是大多数的情况下，数据可没这么友好。很多时候，数据都是不可分的。这样的话，用上面的方法求解，可能就没办法收敛了。那如何处理不可分的情况呢？</p>

<p>定义变量 $\xi = (\xi_1, \xi_2, &hellip; ,\xi_n)$. 很自然，我们有两种方法去重新定义上面的约束条件：</p>

<p>$$\begin{array}{c}
    y_i( x_i^T \beta + \beta_0 ) \ge M - \xi_i \\
    y_i( x_i^T \beta + \beta_0 ) \ge M ( 1 - \xi_i ) \\
    \forall i, \xi_i \ge 0, \sum_{\xi} \le constant
\end{array}$$
第一个会产生非凸解，而第二个的解是凸的。所以，一般我们都是优化第二个方程。</p>

<p>根据定义，我们可以得到:</p>

<p>$\begin{array}{c}
  \min_{\beta, \beta_0}  \frac{1}{2} ||\beta||^2  \\
  subject \ to \ {y_i(x_i^{T}\beta + \beta_0) \ge 1 - \xi_i}, \xi_i \ge 0, \forall i \\
 \sum {\xi_i} \le contant .
\end{array}$</p>

<p>这个定义和上面的定义区别不大，就是后面的约束稍微改了一下，也比较容易理解。上面的式子也就等价于：</p>

<p>$\begin{array}{c}
    \min_{\beta, \beta_0}  \frac{1}{2} ||\beta||^2   + C \sum {\xi_i} \\
    subject \ to \ {y_i(x_i^{T}\beta + \beta_0) \ge 1 - \xi_i}, \xi_i \ge 0, \forall i.
\end{array}$</p>

<p>这一步稍微比较难理解一点，以后再说。这样得到拉格朗日方程是：</p>

<p>$$L_P = \frac{1}{2} || \beta ||^2 + C\displaystyle \sum^N_{i=1} \xi_{i}-
\displaystyle \sum^N_{i=1} \alpha_i [y_i( x^T_i \beta + \beta_0 ) - (1 - \xi_i) ] - \displaystyle \sum^N_{i=1} \mu_i \xi_i$$</p>

<p>对方程中的每个变量求偏导，我们可以得到：</p>

<p>$$\begin{array}{c}
\beta = \displaystyle \sum^N_{i=1} \alpha_i y_i x^T_i \\
0 = \displaystyle \sum^N_{i=1} \alpha_i y_i \\
\alpha_i = C - \mu_i
\end{array}$$</p>

<p>代入Prime中，我们可以得到dual：</p>

<p>$$ \begin{array}{c}
    L_D = \displaystyle \sum^N_{i=1} \alpha_i - \displaystyle \sum^N_{i=1} \sum^N_{k=1} \alpha_i \alpha_k y_i y_k x^T_i x_k \\
    subject \ to \ {\displaystyle \sum^N_{i=1} \alpha_i y_i = 0 \  and \ \alpha_i \ge 0}
 \end{array}
$$</p>

<p>另外，方程在满足TTK conditions的时候，Prime和Dual相等，这也就是我们要求的解。于是有：</p>

<p>  $$\begin{array}
  \ \alpha_i [y_i (x_i^T \beta + \beta_0) - (1 -\xi_i) ] = 0, \forall i \\
  \mu_i \xi_i = 0 \\
  y_i (x_i^T \beta + \beta_0) - (1 -\xi_i)  \ge 0, \forall i
  \end{array}$$</p>

<h3>如何求解</h3>

<ul>
<li><p>Interior Point Method
这个是优化中最常用的方法。不过我不是很熟悉。不做过多的介绍。</p></li>
<li><p>SMO
这个是微软研究出来的算法。SMO也比较简单，比较容易理解。根据定义：$\sum^N_{i=1} \alpha_i y_i = 0$。因此，如果每一次，我们固定一个$\alpha_i$，优化其中一个$\alpha_j$，其他$\alpha_k$都看成常量，这样我们就可以优化$\alpha_j$了。具体的算法可以参考SMO的论文。</p></li>
<li><p>Hinge Loss
不论是IPM还是SMO似乎都和平常的优化不太一样，有没有一种算法是基于Gradient Decsent的呢？有的，看下面的优化问题：</p></li>
</ul>


<p>  $$\displaystyle \min_{\beta, \beta_0} \displaystyle \sum_{i = 1}^N [ 1 - yf(x_i)]_{+} + \frac{\lambda}{2}|| \beta||^2$$</p>

<p>  这个优化问题和上面的SVM是一样的。一样的？真的假的？可以证明。而上面的问题，
  对我们来说，就比较简单了。直接使用求导，使用LBFGS求解就可以了：</p>

<p>  $$\begin{array}{c}
    \min_{\beta, \beta_0}  \frac{1}{2} ||\beta||^2   + C \sum {\xi_i}\\
    subject \ to \ {y_i(x_i^{T}\beta + \beta_0) \ge 1 - \xi_i}, \xi_i \ge 0, \forall i.
  \end{array}$$</p>

<p>  证明：很简单的证明，只要稍微整理一下约束条件就可以了。</p>

<h3>实现？</h3>
</div>


<!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="/blog/2015/05/22/svmjian-dan-jie-shao" data-title="SVM算法介绍" data-url="http://bigbear2017.github.io/blog/2015/05/22/svmjian-dan-jie-shao/"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
    var duoshuoQuery = {short_name:"bigbear2017"};
    (function() {

        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0]
         || document.getElementsByTagName('body')[0]).appendChild(ds);
     })();
 </script>
 <!-- 多说公共JS代码 end -->


<div class="sharing">
  
  
  
</div>
</article>
</div>
	<footer id="footer" class="inner"><script type="text/x-mathjax-config"> 
MathJax.Hub.Config({
	tex2jax: {
		inlineMath: [ ['$','$'] ],
		displayMath: [ ['$$','$$'] ],
	},
	processEscapes: true
});
</script>
<!--script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"--><!--/script-->
<script type="text/javascript" src="http://www.bigbear2017.com/mathjax/MathJax.js?config=default"></script>
<div class="aboutme">
<hr>
  <div class="profile">
    <a href="http://twitter.com/"><img class="icon" src=""></a>

    <h3>Cao Nannan</h3>
    <p><a href="http://twitter.com/">@</a></p>

    <div class="codes">
      <h4>Codes</h4>
      <div class="github"><a href="https://github.com/bigbear2017">github.com/bigbear2017</a></div>
    </div>
  </div>

  <div class="float_clear"></div>

</div>

Copyright &copy; 2016
 Cao Nannan 
<br>
Powered by Octopress.



</footer>
	

</body>
</html>
