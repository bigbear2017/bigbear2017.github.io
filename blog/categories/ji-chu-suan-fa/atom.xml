<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 基础算法 | To Be Your Salt]]></title>
  <link href="http://bigbear2017.github.io/blog/categories/ji-chu-suan-fa/atom.xml" rel="self"/>
  <link href="http://bigbear2017.github.io/"/>
  <updated>2016-06-02T00:04:05+08:00</updated>
  <id>http://bigbear2017.github.io/</id>
  <author>
    <name><![CDATA[Cao Nannan]]></name>
    <email><![CDATA[sei_michael@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[GradientBoosting算法介绍]]></title>
    <link href="http://bigbear2017.github.io/blog/2015/11/24/gradientboostingsuan-fa-jie-shao/"/>
    <updated>2015-11-24T23:45:10+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2015/11/24/gradientboostingsuan-fa-jie-shao</id>
    <content type="html"><![CDATA[<h3>算法介绍</h3>

<p>由AdaBoosting算法，我们知道了什么是Boosting算法，算法大概的结构如下:</p>

<p>$$ G(x) = \sum_{i = 1}^{M} \alpha_i g_i( x ) $$</p>

<p>其中$g_i(x)$是弱分类器，$\alpha_i$是每个分类器的权重。在每一步中，AdaBoosting利用那些被分错类的点，然后增加权重，重新训练一个新的分类器，并且计算分类器的权重。
那什么是Gradient Boosting呢？类似于AdaBoosting, Gradient Boosting也是一种Boosting的算法，结构和上面一样。只是Gradient Boosting是利用Gradient来重新训练一个新的分类器，然后计算权重。</p>

<h3>算法理解</h3>

<h5>1. 简单的Boosting</h5>

<p>如果我们只有一个简单的分类器$g(x)$，不用AdaBoosting，如何进行Boosting呢？
可能最简单的是用Residual来进行训练，过程如下:</p>

<blockquote><p>for j = 1 to M</p>

<ul>
<li><p>$ r_{ji} = ( y_i - G_{j-1}(x_i) ) $ //r_j should be a vector, size of N</p></li>
<li><p>fit $g_j(x)$ to $r_j$</p></li>
<li><p>$\alpha_j = \arg \min_{\alpha} \sum_{i = 1}^n L( y_i, G_{j-1} ( x_i ) + \alpha * g_j(x_i) )  $</p></li>
<li><p>$G_j(x) = G_{j-1}(x) + \alpha_j g_j(x) $</p></li>
</ul>
</blockquote>

<h5>2. 使用Gradient</h5>

<p>上面的每一步，我们使用了一个Residual $(y_i - G_{j-1}(x_i) )$ 来重新训练模型。这个表面上看是挺好的，每一次我们都在逼近$y$。但实际上，我们真的在减少Loss吗？假设我们使用Squared Error作为Loss. 那么: $L=  \sum_{i=1}^N ( y_i - \sum_{j=1}^M g_j(x_i) )^2$ 。这个是不是让我们想到Least Squared Regression呢？里面的$\beta$是如何更新的呢？因为是minimize loss，所以，我们是每次加上负梯度，更新如下:
$\beta_{j+1} = \beta_{j} - \frac{\partial L}{\partial \beta }$</p>

<p>看到这个，是不是我们好像似乎明白了什么。假如，我们把每一个$g_i(x)$都看成变量，如果我们想$G(x)$更接近$y$，那我们应该如何更新呢？
$G(x) = G (x) - \frac{\partial L}{\partial G(x)}$
从这个角度，我们再来理解$y_i - G_j(x_i)$的话，就可以知道它其实就是$L = (y - G(x))^2$的负梯度($-\frac{\partial L} {\partial G(x)} = - ( y - G(x) ) * -1 = y - G(x) $).</p>

<h4>其他梯度</h4>

<h5>1. Absolute Loss</h5>

<p>Absolute loss的定义如下:</p>

<p>$$ L(y, G) = |y - G|$$</p>

<p>梯度如下:
$$ \frac{\partial L}{\partial G} =
\begin{cases}
y - G(x) &amp; if y \ge G(x) \\\
G(x) - y &amp; otherwise
\end{cases} $$</p>

<h5>2. Huber Loss</h5>

<p>$$ L_\delta(y, f(x)) =
\begin{cases}
 \frac{1}{2}(y - f(x))^2                   &amp; \textrm{for} |y - f(x)| \le \delta, \\\
 \delta\, |y - f(x)| - \frac{1}{2}\delta^2 &amp; \textrm{otherwise.}
\end{cases} $$</p>

<p>所以
$$
\frac{\partial L_\delta(y, G(x))} {\partial G} = \begin{cases}
y - G(x)                 &amp; \textrm{for} |y - G(x)| \le \delta, \\\
 \delta sign( y - f(x) )  &amp; \textrm{otherwise.}
\end{cases} $$</p>

<h4>算法过程</h4>

<blockquote><p>Input: training set ${(x_i, y_i)_{i=1}^n}$</p>

<p>初始化learner function $G_0$为常数</p>

<p>for j = 1 to M</p>

<p> &lsquo;  &rsquo;  $ r_{ji} = -\frac{\partial L} {\partial G_{j-1}(x_i)} $ //$r_j$ should be a vector, size of N</p>

<p> &lsquo;  &rsquo;  fit $g_j(x)$ to $r_j$</p>

<p> &lsquo;  &rsquo;  $\alpha_j = \arg \min_{\alpha} \sum_{i = 1}^n L( y_i, G_{j-1} ( x_i ) + \alpha * g_j(x_i) )  $</p>

<p> &lsquo;  &rsquo;  $G_j(x) = G_{j-1}(x) + \alpha_j g_j(x) $</p></blockquote>

<p>初始化learner function $F_0$为常数
计算negative gradient
以negative gradient为目标进行训练得到一个简单的learner
找到最优的常数得到新的更强的learner</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AdaBoosting算法介绍]]></title>
    <link href="http://bigbear2017.github.io/blog/2015/09/27/adaboostingsuan-fa-jie-shao/"/>
    <updated>2015-09-27T21:41:04+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2015/09/27/adaboostingsuan-fa-jie-shao</id>
    <content type="html"><![CDATA[<p>Boosting 是一种很有意思的算法。在没有SVM之前，今天就让我们来一同回顾一下这个经典的算法吧。</p>

<h2>1. 模型定义</h2>

<p>假设模型的输入为$X \in R^p$，输出为$ Y \in (-1, 1)$，输入和输出都有N个点。我们手上呢，有一个很简单的classfier，$G(x)$，他实在不怎么样，可能就比随机猜要好一点点。我们如何利用这个简单的分类器，对数据进行很好的分类呢？AdaBoosting就提出一种很好的办法: 将弱的分类器组合起来，然后我们形成一个强的分类器。这个想法很好，但是我们真的可以把分类器组合起来吗？首先怎么把分类器组合起来呢？直接用余数来训练吗？这样组合起来，分类器变的更糟糕，怎么办？AdaBoosting巧妙地利用了每一个分类器的优点，并且组合了起来。结果也是相当地惊人，在它刚刚被提出来的时候，几乎是当时最好的算法。首先，组合分类器的定义如下:</p>

<p>$$ G(x) = sign ( \sum_{i =1 }^{M} \alpha_m G_m(x)) $$</p>

<p>那如何得到每一个弱分类器的权重呢？AdaBoosting 是利用了每个分类器的错误率来进行组合的，每一次用$G(x)$来分类的错误率就定义如下:</p>

<p>$$ E = \frac{1}{N} \sum_{i=1}^{N} I(y_i \ne G(x_i)$$</p>

<h2>2.模型训练</h2>

<p>Algorithm AdaBoost</p>

<blockquote><ol>
<li>Initialize  $ \omega_i = \frac{1}{N} $,  i = 1, 2, &hellip; N</li>
<li>For m = 1 to M:</li>
<li>(a) Fit a classifier G(x) to training data using weights $\omega_i$</li>
<li>(b)Compute</li>
<li>$$ err_m = \frac{\sum_{i=1}^{N} \omega_m I( y_i \ne G_m(x))} {\sum_{i = 1}^{N} \omega_i}  $$</li>
<li>&copy; Compute $ \alpha_m = \log(\frac{( 1 - err_m)} {err_m} ) $</li>
<li>(d) Update $\omega_i  = \omega_i \exp(\alpha_m I( y_i \ne G_m(x_i) )$</li>
<li>Output  $ G(x) = sign[ \sum_{m=1}^{M} \alpha_m G_m(x) ] $</li>
</ol>
</blockquote>

<h2>3. 算法解读</h2>

<ol>
<li>为什么 $\alpha_m$定义是 $\log(\frac{( 1 - err_m)} {err_m} )$ ?</li>
</ol>


<p>首先$err_m$的取值范围是$(0, 1)$。如果分类器$G_m(x)$的错误率越高，那么$err_m$就越接近于1, 否则就越接近于$0$。而根据常识来说，如果一个分类器很差劲，错误率越高呢，我们就希望他的权重就越低，越接近于0。而这样的话，就让我们想到了一个很常见的函数sigmod fuction。就让我们来证明这个来历吧。</p>

<p>$$ \begin{matrix}
err_m =  \frac{1}{1 + \exp^{\alpha}} \\
=> 1 + \exp^{\alpha} = \frac{1}{err_m} \\
=> \exp^{\alpha} = \frac{1}{err_m} - 1 \\
=> \alpha = \log(\frac{( 1 - err_m)} {err_m} )
\end{matrix} $$</p>

<p>上面并不是logistic function(lf)，而是一个按照y轴对称的lf，因为如果直接用lf的话，err越大，权重也会越大了。</p>

<ol>
<li>为什么每次要更新$\omega_i$ ?
对于 $\omega_i$ 的更新，我们可以进行这样的解读:

<blockquote><p>if $y_i == G(x_i)$ $\omega_i == \omega_i$
else $\omega_i = \omega_i * \frac{1- err_m}{err_m}$</p></blockquote></li>
</ol>


<p>对于 $\frac{1- err_m}{err_m}$，我们知道$err_m$越大，值越小。也就是说，错误率越高，我们的就权重越低。这是可以理解的，如果$err_m$很大，说明分类器不好。被这样一个分类器给分错了，那我们可能觉得没关系。但是如果$err_m$很小，说明分类器很好，被一个很好的分类器分错了，说明这个点，我们希望在下次的时候，可以被其他的分类器补充。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NeronNetwork算法介绍]]></title>
    <link href="http://bigbear2017.github.io/blog/2015/06/22/neronnetworksuan-fa-jie-shao/"/>
    <updated>2015-06-22T20:05:00+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2015/06/22/neronnetworksuan-fa-jie-shao</id>
    <content type="html"><![CDATA[<p>最近深度学习实在太火了，火到各个公司都在搞实验室。但是深度学习的基础，还是从neron network，BP，这样的算法开始。
就简单回顾一下最简单的NN吧。</p>

<h3>1. 算法介绍</h3>

<p>我们假设只有一层的隐藏层，并且大小为M，输入$X \in R^p$，输出$Y \in R^K$，输出$Y$属于$K$ classes。所以，我们可以得到下面的定义:</p>

<p>$$ \begin{matrix}
D_m = \rho ( \alpha_m0 + \alpha_m^T X ) , m = 1, 2, &hellip; M \\
T_k = \beta_k0 + \beta_k^T D, k = 1, 2, &hellip; K \\
f_k(X) = g_k( T ) , k = 1,2, &hellip; K
\end{matrix}$$</p>

<p>其中$D = (D_1, D_2, &hellip; , D_M)$, $\rho = \frac{1}{ 1 + \exp^{(-v)} }$是sigmod function, $g_k$可以是identity function，或者是softmax function $\frac{exp^{T_k}}{\sum_{l = 1}^{K} exp^{T_l}}$.</p>

<h3>2. 求解</h3>

<p>训练采用的back propagation的方式，就是先利用参数计算结果，然后根据结果计算梯度，根据梯度再优化参数。
根据上面的定义，我们可以得到loss function $R(\alpha, \beta)$如下:</p>

<p>$$
R(\alpha, \beta) = \sum_{k = 1}^K \sum_{n = 1}^N ( y_{ik} - f_k( x_i ) ) ^2
$$</p>

<p>其中$N$是样本数, $K$是样本的class的数目. 所以对$\alpha, \beta$分别求偏导，我们可以得到:</p>

<p>$$\begin{matrix}
\frac{\partial R_i}{\partial \beta_{km} } = -2(y_{ik} - f_k( x_i ) ) g^{\prime}(T_k) D_{mi}   \\
\frac{\partial R_i}{\partial \alpha_{mp}} =  -\sum_{k=1}^{K}2(y_{ik} - f_k( x_i ) ) g^{\prime}(T_k) \beta_{k}\rho^{\prime}(\alpha_m^T x_i) x_{ip}
\end{matrix}
$$</p>

<p>根据上面的式子，我们就可以利用gradient descent的方式，来训练模型了.</p>

<p>$$ \begin{matrix}
\beta^{r+1}_{km} = \beta^{r}_{km}  - \gamma_r \frac{\partial R_i}{\partial \beta_{km} } \\
\alpha^{r+1}_{mp} = \alpha^{r}_{mp}  - \gamma_r \frac{\partial R_i}{\partial \beta_{mp} }
\end{matrix}$$</p>

<h3>3. 向量化</h3>

<p>上面的式子，实在看着太繁琐了，我们还是对其进行向量化，可能看着更简单，实现也更方便一些.
待续</p>

<h3>4. 注意点</h3>

<h4>1. 输入的起始点，不可以全部是零。因为全部是零的话，就永远全部是零，无法训练。也不可以过大，结果会不太好。最好的是随机靠近0的数。</h4>

<h4>2. 隐藏层多一点比较好，可以表达的非线性的能力就越强一点</h4>

<h4>3. 加正则防止过拟合。</h4>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SVM算法介绍]]></title>
    <link href="http://bigbear2017.github.io/blog/2015/05/22/svmjian-dan-jie-shao/"/>
    <updated>2015-05-22T19:35:43+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2015/05/22/svmjian-dan-jie-shao</id>
    <content type="html"><![CDATA[<p>在没有深度学习之前，SVM非常火。最近似乎很少有人再用svm了，不过还是让我们来回顾一下这个经典的算法吧。</p>

<h3>1. Perceptron</h3>

<p>说到svm，我们可以先从最简单的perceptron说起。Percetron是最简单的一种分类器。给n个点(X, Y), 假设
$X \in R^p, Y \in (0, 1)$，如何得到一个分类器$f(x) = \beta * x + \beta_0$，使得所有被错误分类的点，到分类线的距离最小。我们希望说，如果被错误分类了，也不要错的太厉害。这样的话，我们就可以得到下面的式子:</p>

<p>$$
D( \beta, \beta_0 ) = - \sum_{i} y_i( \beta x_i + \beta_0 )
$$</p>

<p>根据gradient decent，随机初始化一个$\beta$，然后每次更新$\beta$如下：</p>

<p>$$ (\beta, \beta_0 )_{n+1}= ( \beta, \beta_0 )_n + \rho * (y_i x_i, y_i )$$</p>

<p>在扫描几遍数据之后，我们得到一个稳定的$\beta, \beta_0$就可以了。</p>

<h3>2. Best Seperating Hyperplane</h3>

<p>在上面求解的过程中，我们会发现，每次得到的解都是不一样的。随着初始化，数据的输入，都会发生变化。那怎么才是最好的seperating hyperplane呢？SVM提出了一个观点，任何一个类中的点到分类线的距离都是最大的，这个就是最好的。
所以从上面，我们可以得到:</p>

<p>$$
\begin{matrix}<br/>
\max_{\beta  \ \ \beta_0 } M  \\
subject \ \ to \ \ \frac{y_i ( \beta x_i + \beta_0  )} {||\beta||} \ge M
\end{matrix}<br/>
$$</p>

<p>从上面的式子，我们知道，对于 $ || \beta || $，我们取任何值，它都不会影响点到线的距离，因为我们已经做了归一化了.那么我们就取$ || \beta || = \frac{1}{M} $ 好了, 这样我们可以得到:</p>

<p>$$
\begin{matrix}<br/>
\max_{\beta  \ \ \beta_0 }  \frac{1}{ ||\beta || } \\
subject \ \ to \ \ y_i ( \beta x_i + \beta_0  ) \ge 1
\end{matrix}<br/>
$$</p>

<p>稍微转化一下，我们可以得到下面的式子:
$$
\begin{matrix}<br/>
\min_{\beta  \ \ \beta_0 }  \frac{1}{2}{ ||\beta ||^2 } \\
subject \ \ to \ \ y_i ( \beta x_i + \beta_0  ) \ge 1
\end{matrix}<br/>
$$</p>

<p>这样的话，我们就可以对上面的式子进行求解了，根据拉格朗日算法，我们可以得到:</p>

<p>$$
L_P = \frac{1}{2}{ ||\beta ||^2 } - \sum_{ i = i}^{n} \alpha_i [ y_i ( \beta x_i + \beta_0  ) - 1 ]
$$</p>

<p>对$\beta, \beta_0$，分别求偏导，我们可以得到:</p>

<p>$$
\begin{matrix}
\frac{\partial L_P} {\partial \beta } = \beta - \sum_{i = 1}^{n} \alpha_i y_i x_i = 0 \\
\frac{\partial L_P} {\partial \beta_0 } = \sum_{i=1}^{n} \alpha_i y_i = 0
\end{matrix}<br/>
$$</p>

<p>代入原来的式子，我们可以得到一个dual的形式:</p>

<p>$$
\begin{matrix}
L_D =  \sum_{i = 1}^{n} \alpha_i - \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j x_i^T x_j
\\
subject \ \ to \ \ \alpha_i \ge 0 \ and \ \sum_{i=1}^{n} \alpha_i y_i  = 0
\end{matrix}
$$</p>

<p>从上面的式子，我们可以看到：</p>

<ol>
<li>如果$\alpha_i >0 $，那么$y_i (\beta x_i + \beta_0) = 1$，那么点就在边界上面</li>
<li>如果$\alpha_i = 0 $，那么点不在边界上面</li>
</ol>


<h3>3. 如何求解</h3>

<p>经过上面的变换，我们可以得到一个dual，然后使用 SMO 算法，或者其他算法我们都可以得到解。但是都是比较麻烦的。我们可不可以像其他算法一样，使用最简单的Gradient Descent 来求解呢？如果可以的话，那样会简单很多。(待续)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Newton Method, BFGS和LBFGS算法介绍]]></title>
    <link href="http://bigbear2017.github.io/blog/2014/09/12/newton-method/"/>
    <updated>2014-09-12T21:49:55+08:00</updated>
    <id>http://bigbear2017.github.io/blog/2014/09/12/newton-method</id>
    <content type="html"><![CDATA[<p>本文主要介绍Newton Method，BFGS, LBFGS 算法，最重要的是算法推导的过程，以及这三个算法的比较。</p>

<h2>Newton Method</h2>

<p>假设$y = f(x)$，为了求方程$f(x) = 0$的解，我们可以用下面迭代的方式：</p>

<p>$$ x_{n+1} = x_n - \frac{ f(x_n) } { f^{\prime} (x_n) } $$</p>

<p>比如$ f(x) = x^{2} - 5 $，我们可以这么来求：</p>

<p>$$ x_1 = 5, f(5) = 20, f^{\prime}(5) = 10, x_2 = 5 - 20 / 10 = 3 $$
$$x_2 = 3, f(3) = 4, f^{\prime}(3) = 6, x_3 = 3 - 4/6 = 7/3 $$
$$x_3 = 7/3, f(7/3) = 4/9, f^{\prime}(7/3) = 14/3, x_4 = 7/3 - 2/21 = 47/21 $$</p>

<p>这样$ 47/21 $ 已经非常接近5的平方根了。为什么可以这么计算呢？证明如下：
根据泰勒展开式，$ y = f(x) $ 在 $x_n$处的展开为：</p>

<p>$$ y = f^{\prime}(x_n) ( x_{n+1} - x_n ) + f(x_n) $$</p>

<p>如果$ y = 0 $, 那么可得如下结果：</p>

<p>$$ x_{n+1} = x_n - f(x_n) / f^{\prime}(x_n) $$</p>

<p>在许多机器学习的算法中，我们经常可能碰到的是另外一种形式，比如我们想最大化log likelihood函数。
这个时候，我们会对函数先求一个导，导数$f^{\prime}(x) = 0$的地方一般就是极值点。因此这个时候，我们想求的是
$ f^{\prime}(x) = 0 $，而不是$ f(x) = 0 $。如果求$ f^{\prime}(x) = 0 $，则方程的解如下：</p>

<p>$$ x_{n+1} = x_n - \frac{f^{\prime}(x)}{f^{\prime \prime}(x)} $$</p>

<p>但是因为这个方法要求函数的二阶导数，就像在logistic regression中说的一样。求出来的Hessian Matrix实在太大，
求逆也非常的麻烦。因此就有人想到了更为简单的算法，LBFGS就是其中一个比较好的方法，我们先从BFGS算法说起。</p>

<h2>BFGS</h2>

<p>BFGS算法是由Broyden, Fletcher, Goldfarb 和 Shanno四个人共同提出的。这里我们把他们的名字列出来，纪念一下。:)
由最简单的Line Search的方法，我们知道对于求方程$ f(x) = 0 $的解，我们可以进行如下迭代：</p>

<p>$$ x_{k+1} = x_k + \alpha_k \rho_k $$</p>

<p>其中 $\alpha_k$ 为步长，$\rho_k$为迭代方向。根据泰勒展开，我们可以把$f(x)$写成下面格式：</p>

<p>$$ f(x_{k+1}) = f(x_k) + f^{\prime}(x_k)(x_{k+1} - x_k) + \frac{1}{2} (x_{k+1} - x_k)^{T} f^{\prime \prime}(x_k) ( x_{k+1} - x_k ) $$</p>

<p>上面的函数，也可以写成$ \rho $ 的函数，如下：</p>

<p>$$ m_k(\rho) = f_k + f^{\prime T}_k \rho + \frac{1}{2} \rho^{T} B_k \rho $$</p>

<p>其中$f_k$为$f(x_k)$, $f^{\prime}_k$为$f(x)$在$x_k$处的导数。根据上面的式子，当$\rho = 0$的时候，</p>

<p>$$ m_k(0) =  f_k, m^{\prime}_k (0) = f^{\prime T}_k $$</p>

<p>因此在任何一点$x_k$，$f(x_k)$都可以写成$m_k$的形式，方程的曲线也应该是相同的。根据上面的定义，我们可以得到方程在
$x_{k+1}$处，$m_{k+1}$的表达式为:</p>

<p>$$ m_{k+1}(\rho) = f_{k+1} + f^{\prime T}_{k+1} \rho + \frac{1}{2} \rho^{T} B_{k+1} \rho $$</p>

<p>根据这个表达式，$m_{k+1}$ 这个表达式在$x_k$处的导数为:</p>

<p>$$ f^{\prime}(x_k) = m^{\prime}_{k+1}(-\alpha_k \rho_k) = f^{\prime T}_{k+1} - \alpha_k B_{k+1} \rho_k $$</p>

<p>所以得到：</p>

<p>$$ B_{k+1} \alpha_k \rho_k = f^{\prime}_{k+1} - f^{\prime}_k $$</p>

<p>为了简化，定义如下:</p>

<p>$$ s_k = x_{k+1} - x_k = \alpha_k \rho_k, y_{k} = f^{\prime}_{k+1} - f^{\prime}_k $$</p>

<p>所以上面的结果可以写成:</p>

<p>$$ B_{k+1} s_k = y_k $$</p>

<p>假设$ H<em>{k+1} $为$B</em>{k+1}$的逆矩阵，那么我们可以得到:</p>

<p>$$ H_{k+1} y_k = s_k $$</p>

<p>利用$H_k$，$H_{k+1}$可以通过下面的方式获得:</p>

<p>$$ \min_H || H - H_{k+1} || $$</p>

<p>subject to $ H = H^{T}, Hy_k = s_k $.这样我们可以得到$ H_{k+1} $的解为：</p>

<p>$$ H_{k+1} = (I - p_k s_k y^{T}_k)H_k(I-p_k y_k s^{T}_k) + p_k s_k s^{T}_k$$</p>

<p>其中$p_k = \frac{1}{y^{T}_k s_k}$.到这里BFGS的解法就算明白了。相对于Newton Method，BFGS方法不用
计算Hessian Matrix的逆矩阵。但是它仍然要计算一个matrix，所以内存的占用仍然是一样的。具体算法如下：
<code>
while || f^{\prime}_k|| &gt; \epsilon
          compute search direction
                  p_k = -H_k f^{\prime}_k
          x_{k+1} = x_k + \alpha_k p_k where \alpha_k is computed from a line search
              procedure to satisfy the Wolfe conditions.
          Define s_k = x_{k+1} - x_k and y_k = f^{\prime}_{k+1} - f^{\prime}_k
          Compute H_{k+1} with above formula.
          k = k + 1
end( while )
</code></p>

<h3>LBFGS</h3>

<p>根据上面的推导，BFGS算法可以写成下面的格式:</p>

<p>$$ x_{k+1} = x_{k} - \alpha_k H_k f^{\prime}_k(x) $$</p>

<p>其中$\alpha_k$为步长，$H_k$可以写成下面的格式:</p>

<p>$$ H_{k+1} = V^{T}_k H_{k} V_k + p_k s_k s^{T}_k $$</p>

<p>其中$p_k = \frac{1}{y^{T}_k s_k}$, $V_k = I - p_k y_k s^{T}_k $, and $s_k = x_{k+1} - x_k$,$y_k = f^{\prime}_{k+1} - f^{\prime}_{k}$.</p>

<p>由于计算$H_{k+1}$相当的麻烦，我们可以存储$m$个最近的$y_i, s_i$，然后来近似当前的$H_{k+1}$. 根据$H_{k+1}$的定义，我们不难发现这是一个递归的定义。在所有计算的过程中，其实就只有$H_0$这么一个矩阵，其他的全部是向量。如果我们使用一个对角矩阵来近似$H_0$，那么所有的计算都是可以用向量来表示。这个也是为什么它叫limited memory BFGS。
<code>
calculate the new direction \rho_k
q = f^{\prime}(x_k)
for i = k-1 ... k-m
  \alpha_i = p_i s^T_i q
  q = q - \alpha_i y_i
end for
r = H_0 q
for i = k-m ... k-1
  \beta = p_i y^T_i r
  r = r + s_i(\alpha_i - \beta)
end for
return r which is H_{k}*f^{\prime}_k
</code></p>

<h2>L-BFGS algorithm</h2>

<pre><code>choose starting point x_0, integer m &gt; 0;
k = 0;
repeat
      Choose H_0
      Compute \rho_k = - H_{k}*f^{\prime}_k
      Compute x_{k+1} = x+k + \alpha_k \rho_k, where \alpha_k is the step length
          which statisfy the Wolfe conditions.
      if k &gt; m
              Discard the vector pair{ s_{k-m}, y_{k-m} } from storage.
      Compute and save s_k = x_{k+1} - x_{k}, y = f^{\prime}_{k+1} - f^{\prime}_k
      k = k+1
</code></pre>

<p>参考资料
<a href="http://en.wikipedia.org/wiki/Newton%27s_method">http://en.wikipedia.org/wiki/Newton%27s_method</a>
Numerical Optimization, Chapter 6, 7.</p>
]]></content>
  </entry>
  
</feed>
